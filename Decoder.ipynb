{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 133.39932250976562\n",
      "100 107.00054168701172\n",
      "200 106.9864730834961\n",
      "300 106.9794692993164\n",
      "400 106.97309875488281\n"
     ]
    }
   ],
   "source": [
    "#F to 12 x 12 x 256\n",
    "#stacked transposed convolutions, separated by ReLUs, with a kernel width of 5 and stride of 2 to upsample to 96 x 96 x 32\n",
    "#1 x 1 convolution to 96 x 96 x 3\n",
    "\n",
    "# torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(17)\n",
    "\n",
    "f_vector = 736 # nn4 model의 output layer 전 마지막 layer dimension\n",
    "landmark_num = 68\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, useCude=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.base_fc = nn.Linear(f_vector, f_vector)\n",
    "        \n",
    "        self.shallow_mlp = nn.Sequential(\n",
    "            nn.Linear(f_vector, 256), # hidden1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128), # hidden2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 68), # out\n",
    "            nn.ReLU())\n",
    "                \n",
    "        self.texture_fc = nn.Linear(f_vector, 256 * 12 * 12)\n",
    "        \n",
    "        self.tConv1 = nn.ConvTranspose2d(256, 128, 5, stride=2, padding=2)\n",
    "        self.tConv2 = nn.ConvTranspose2d(128, 64, 5, stride=2, padding=2)\n",
    "        self.tConv3 = nn.ConvTranspose2d(64, 32, 5, stride=2, padding=2)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 3, kernel_size=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU())\n",
    "\n",
    "    # x := 논문에서 1024, 우리는 736 vector\n",
    "    def forward(self, x):\n",
    "        feature_vec = self.base_fc(x)\n",
    "        \n",
    "        x1 = self.shallow_mlp(feature_vec)\n",
    "        y1 = self.shallow_mlp(feature_vec)\n",
    "        \n",
    "        texture_out = self.texture_fc(feature_vec)\n",
    "        texture_out = texture_out.view(1, 256, 12, 12)\n",
    "        texture_out = F.relu(self.tConv1(texture_out, output_size=[None, None, 24, 24]))\n",
    "        texture_out = F.relu(self.tConv2(texture_out, output_size=[None, None, 48, 48]))\n",
    "        texture_out = F.relu(self.tConv3(texture_out, output_size=[None, None, 96, 96]))\n",
    "        texture_out = self.conv(texture_out)\n",
    "        return x1, y1, texture_out\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# input : 736 dimension\n",
    "x = Variable(torch.randn(1, 1, 1, f_vector))\n",
    "\n",
    "# ground_truth_textures := actual input image\n",
    "#y = Variable(ground_truth_textures, requires_grad=False)\n",
    "ground_truth_textures = Variable(torch.randn(1, 3, 96, 96), requires_grad=False)\n",
    "landmark_x = Variable(torch.randn(1, landmark_num))\n",
    "landmark_y = Variable(torch.randn(1, landmark_num))\n",
    "\n",
    "net = Decoder()\n",
    "\n",
    "loss_fn_landmark = torch.nn.MSELoss(size_average=False)\n",
    "loss_fn_texture = torch.nn.L1Loss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    x_pred, y_pred, texture_pred = net(x)\n",
    "\n",
    "    loss_x = loss_fn_landmark(x_pred, landmark_x)\n",
    "    loss_y = loss_fn_landmark(y_pred, landmark_y)\n",
    "    loss_texture = loss_fn_texture(texture_pred, ground_truth_textures)\n",
    "    \n",
    "    total_loss = loss_x + loss_y + loss_texture\n",
    "    if (t % 100 == 0):\n",
    "        print(t, total_loss.data[0])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
